{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7334d5",
   "metadata": {},
   "source": [
    "# Information Extraction (IE)\n",
    "### Goal of lesson\n",
    "- What is Information Extraction\n",
    "- Extract knowledge from patterns\n",
    "- Word representation\n",
    "- Skip-Gram architecture\n",
    "- To see how words relate to each other (this is surprising)\n",
    "\n",
    "### What is Information Extraction (IE)\n",
    "- the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents ([wiki](https://en.wikipedia.org/wiki/Information_extraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7c9c6",
   "metadata": {},
   "source": [
    "### Extract knowledge from patterns\n",
    "- Given data knowledge that is fit together - find patterns\n",
    "- Example\n",
    "    - Knowledge given:\n",
    "        - Amazon (1992)\n",
    "        - Facebook (2004)\n",
    "    - Pattern (template) found:\n",
    "        - \"When {company} was founded in {year},\"\n",
    "- This is a simple, but very powerful approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d090354",
   "metadata": {},
   "source": [
    "> #### Programming Notes:\n",
    "> - Libraries used\n",
    ">     - [**pandas**](https://pandas.pydata.org) - a data analysis and manipulation tool\n",
    ">     - [**re**](https://docs.python.org/3/library/re.html) regular expressions\n",
    "> - Functionality and concepts used\n",
    ">     - [**CSV**](https://en.wikipedia.org/wiki/Comma-separated_values) file ([Lecture on CSV](https://youtu.be/LEyojSOg4EI))\n",
    ">     - [**read_csv()**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) read a comma-separated values (csv) file into **pandas** DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980da212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9bcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('files/books.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ea5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = books.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2128a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1984', 'George Orwell'], ['The Help', 'Kathryn Stockett']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5994835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/penguin.html') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dddf0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.replace('\\n', ' ').replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0652325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984 - George Orwell\n",
      "-: ge-orwell-with-a-foreword-by-thomas-pynchon/\">1984</a></h2>   <h2 class=\"author\">by George Orwell</h\n",
      "-: eword-by-thomas-pynchon/\">1984</a></h2>   <h2 class=\"author\">by George Orwell</h2>    <div class=\"de\n",
      "-: hon/\">1984</a></h2>   <h2 class=\"author\">by George Orwell</h2>    <div class=\"desc\">We were pretty c\n",
      "The Help - Kathryn Stockett\n",
      "-: /the-help-by-kathryn-stockett/\">The Help</a></h2>   <h2 class=\"author\">by Kathryn Stockett</h2>    <\n",
      "-: -stockett/\">The Help</a></h2>   <h2 class=\"author\">by Kathryn Stockett</h2>    <div class=\"desc\">Thi\n"
     ]
    }
   ],
   "source": [
    "for val1, val2 in book_list:\n",
    "    print(val1, '-', val2)\n",
    "    for i in range(0, len(corpus) - 100, 20):\n",
    "        pattern = corpus[i:i + 100]\n",
    "        if val1 in pattern and val2 in pattern:\n",
    "            print('-:', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedf8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0da682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/\">',\n",
       " '</a></h2>\\\\ \\\\ \\\\ <h2\\\\ class=\"author\">by\\\\ ',\n",
       " '</h2>\\\\ \\\\ \\\\ \\\\ <div\\\\ class=\"desc\">')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = re.escape('/\">')\n",
    "middle = re.escape('</a></h2>   <h2 class=\"author\">by ')\n",
    "suffix = re.escape('</h2>    <div class=\"desc\">')\n",
    "prefix, middle, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82676895",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = f\"{prefix}(.{{0,50}}?){middle}(.{{0,50}}?){suffix}\"\n",
    "results = re.findall(regex, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79944531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('War and Peace', 'Leo Tolstoy'),\n",
       " ('Song of Solomon', 'Toni Morrison'),\n",
       " ('Ulysses', 'James Joyce'),\n",
       " ('The Shadow of the Wind', 'Carlos Ruiz Zafon'),\n",
       " ('The Lord of the Rings', 'J.R.R. Tolkien'),\n",
       " ('The Satanic Verses', 'Salman Rushdie'),\n",
       " ('Don Quixote', 'Miguel de Cervantes'),\n",
       " ('The Golden Compass', 'Philip Pullman'),\n",
       " ('Catch-22', 'Joseph Heller'),\n",
       " ('1984', 'George Orwell'),\n",
       " ('The Kite Runner', 'Khaled Hosseini'),\n",
       " ('Little Women', 'Louisa May Alcott'),\n",
       " ('The Cloud Atlas', 'David Mitchell'),\n",
       " ('The Fountainhead', 'Ayn Rand'),\n",
       " ('The Picture of Dorian Gray', 'Oscar Wilde'),\n",
       " ('Lolita', 'Vladimir Nabokov'),\n",
       " ('The Help', 'Kathryn Stockett'),\n",
       " (\"The Liar's Club\", 'Mary Karr'),\n",
       " ('Moby-Dick', 'Herman Melville'),\n",
       " (\"Gravity's Rainbow\", 'Thomas Pynchon'),\n",
       " (\"The Handmaid's Tale\", 'Margaret Atwood')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaf700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9111b2",
   "metadata": {},
   "source": [
    "### One-Hot Representation\n",
    "- Representation word as a vector with a single 1, and with other values as 0\n",
    "- Maybe not useful to have with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db796b6",
   "metadata": {},
   "source": [
    "### Distributed Representation\n",
    "- representation of meaning distributed across multiple values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e0bd6",
   "metadata": {},
   "source": [
    "### How to define words as vectors\n",
    "- Word is defined by what words suround it\n",
    "- Based on the context\n",
    "- What words happen to show up around it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276ff32",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "- model for generating word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd719e53",
   "metadata": {},
   "source": [
    "### Skip-Gram Architecture\n",
    "- Neural network architecture for predicting context words given a target word\n",
    "    - Given a word - what words show up around it in a context\n",
    "- Example\n",
    "    - Given **target word** (input word) - train the network of which **context words** (right side)\n",
    "    - Then the weights from input node (**target word**) to hidden layer (5 weights) give a representation\n",
    "    - Hence - the word will be represented by a vector\n",
    "    - The number of hidden nodes represent how big the vector should be (here 5)\n",
    "\n",
    "<img src=\"img/word_vectors.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22232a",
   "metadata": {},
   "source": [
    "- Idea is as follows\n",
    "    - Each input word will get weights to the hidden layers\n",
    "    - The hidden layers will be trained\n",
    "    - Then each word will be represented as the weights of hidden layers\n",
    "- Intuition\n",
    "    - If two words have similar context (they show up the same places) - then they must be similar - and they have a small distance from each other representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caacddbd",
   "metadata": {},
   "source": [
    "> #### Programming Notes:\n",
    "> - Libraries used\n",
    ">     - [**numpy**](http://numpy.org) - scientific computing with Python ([Lecture on NumPy](https://youtu.be/BpzpU8_j0-c))\n",
    ">     - [**scipy**](https://www.scipy.org) - open-source software for mathematics, science, and engineering\n",
    "> - Functionality and concepts used\n",
    ">     - [**cosine**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) Compute the Cosine distance between 1-D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417c4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/words.txt') as f:\n",
    "    words = {}\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        row = line.split()\n",
    "        word = row[0]\n",
    "        vector = np.array([float(x) for x in row[1:]])\n",
    "        words[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a8cfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.24722e-01, -2.53714e-01,  2.88594e-01, -8.69330e-02,\n",
       "        3.95457e-01, -2.69500e-03, -1.91515e-01, -1.88049e-01,\n",
       "       -2.30164e-01, -2.29000e-04,  3.20153e-01, -1.72093e-01,\n",
       "        2.45010e-02, -1.95822e-01, -3.53977e-01, -2.22997e-01,\n",
       "        5.02481e-01,  5.25600e-01, -6.74100e-03,  3.70885e-01,\n",
       "        1.07482e-01,  3.64006e-01,  3.02715e-01, -4.74844e-01,\n",
       "        2.98490e-02, -2.78149e-01, -1.11824e-01,  3.38680e-01,\n",
       "       -1.70843e-01,  5.01430e-02, -2.13096e-01,  1.14150e-02,\n",
       "       -4.48091e-01,  4.76869e-01, -2.71355e-01,  4.04440e-02,\n",
       "        2.55024e-01, -1.13249e-01, -4.94210e-02,  1.58128e-01,\n",
       "       -3.11536e-01,  1.44958e-01, -4.19831e-01, -2.98550e-02,\n",
       "       -1.20394e-01,  4.05910e-01,  5.26441e-01, -2.24795e-01,\n",
       "       -1.24436e-01, -1.69059e-01, -6.82700e-03,  1.37938e-01,\n",
       "       -2.27450e-01,  2.71422e-01, -2.96443e-01,  3.18369e-01,\n",
       "        4.04352e-01, -1.27548e-01, -4.99130e-02, -2.87840e-01,\n",
       "       -7.39040e-02, -4.39700e-02, -1.91822e-01, -2.74386e-01,\n",
       "        3.85270e-02,  2.44000e-02, -1.37660e-02, -3.33430e-02,\n",
       "       -1.84430e-01,  7.71390e-02, -1.30915e-01, -4.19860e-02,\n",
       "        1.73335e-01, -1.68000e-04, -1.14695e-01,  2.13130e-02,\n",
       "       -1.11972e-01,  6.00470e-02,  8.85150e-02,  1.09064e-01,\n",
       "       -6.09310e-02,  2.29500e-02,  2.60920e-01,  2.08235e-01,\n",
       "        7.06470e-02,  2.73027e-01,  1.32511e-01,  6.05930e-02,\n",
       "        4.93924e-01, -9.15740e-02,  1.39254e-01,  1.54233e-01,\n",
       "        1.27228e-01,  2.53722e-01, -5.91450e-02,  4.48260e-02,\n",
       "        7.40590e-02, -1.77663e-01,  1.76156e-01,  8.99200e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0037e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def distance(word1, word2):\n",
    "    return cosine(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42f22d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19707422881543946"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(words['king'], words['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18cd8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42088794105426874"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(words['king'], words['pope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9732cdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7674565342428494"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(words['king'], words['card'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f5c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_words(word):\n",
    "    distances = {w: distance(word, words[w]) for w in words}\n",
    "    return sorted(distances, key=lambda w: distances[w])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb60b485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queen',\n",
       " 'king',\n",
       " 'empress',\n",
       " 'prince',\n",
       " 'duchess',\n",
       " 'princess',\n",
       " 'consort',\n",
       " 'monarch',\n",
       " 'dowager',\n",
       " 'throne']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_words(words['king'] - words['man'] + words['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe9652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
